---
layout: article_home
<!-- title: Knowledge Representation for Hybrid and Compositional AI (KRHCAI 2021 Workshop)
subtitle: To be held jointly with the  KR2021 Conference -->
title:  KRHCAI 2021
subtitle: Knowledge Representation for Hybrid & Compositional AI
header:
  theme: dark
  background: '#123'
article_header:
  type: overlay
  theme: dark
  background_color: '#123'
  background_image: false
key: page-index
---

<!-- Just say something about yourself. :+1:

{% highlight javascript %}
(() => console.log('hello, world!'))();
{% endhighlight %} -->

<div style="padding-top: 50px">
    <div>
        AI research over the past few decades has highlighted the strengths and weaknesses of both symbolic and machine learning approaches to AI, with some weaknesses in one being strengths in the other. Much is also known about the fragmentation of the field along these two approaches. However, recent discussions in AI have highlighted the need to integrate both symbolic and sub-symbolic methods, in a hybrid approach to AI, to create novel techniques that leverage both reasoning and learning. 

        This workshop seeks to contribute to this discussion by exploring the following: 
        <ul>
            <li><i>a systems approach to AI,</i></li>
            <li><i>the use of composable AI components,</i></li>
            <li><i>leveraging the best of both symbolic and sub-symbolic techniques in hybrid reasoning and learning architectures.</i></li>
        </ul>
        
        To achieve the above, there is the need to explore novel knowledge representation techniques that allow 
        the seamless flow of computations between symbolic and sub-symbolic AI components.  
        There is also the need to develop new datasets that evaluate the capabilities highlighted above, 
        especially focusing on problems that cannot be solved by end-to-end differentiable neural network 
        architectures or purely symbolic reasoning methods alone. 

    </div>
    <br/>
    <div>Workshop dates: <b>November 3-7, 2021</b></div>
    <br/>
    <a name="speakers"></a>
    <h2>Invited Speakers</h2>
    <br/>
    <div>
        <div class="image-cropper" style="float:left;  margin-right: 20px">
            <img src="assets/images/logo/speakers/gray_alex.jpg" class="rounded" />
        </div>
        <div style="float:left;"">
            <h3 style="margin-bottom: 0px;">Dr. Alexander Gray</h3>
            <span>IBM</span>
            
        </div>
        <div style="clear:both;""></div>
        <!-- <div style="margin-left:20px; margin-top: 30px; color:rgb(53, 17, 17); font-size:12px"><i>Abstract</i></div> -->
        <h4 style="margin-left:20px; font-weight:600; margin-top:20px">Logical Neural Networks: Tow</h4>
        <div style="margin-left:20px; text-align: left;">
            Recently there has been renewed interest in the long-standing goal of somehow unifying the capabilities of both statistical 
            AI (learning and prediction) and symbolic AI (knowledge representation and reasoning).  We introduce Logical Neural Networks, 
            a new neuro-symbolic framework which identifies and leverages a 1-to-1 correspondence between an artificial neuron and a 
            logic gate in a weighted form of real-valued logic.  With a few key modifications of the standard modern neural network, 
            we construct a model which performs the equivalent of logical inference rules such as modus ponens within the message-passing 
            paradigm of neural networks, and utilizes a new form of loss, contradiction loss, which maximizes logical consistency in the 
            face of imperfect and inconsistent knowledge.  The result differs significantly from other neuro-symbolic ideas in that 1) the 
            model is fully disentangled and understandable since every neuron has a meaning, 2) the model can perform both classical 
            logical deduction and its real-valued generalization (which allows for the representation and propagation of uncertainty) 
            exactly, as special cases, as opposed to approximately as in nearly all other approaches, and 3) the model is compositional 
            and modular, allowing for fully reusable knowledge across talks.  The framework has already enabled state-of-the-art results 
            in several problems, including question answering.
        </div>
        <div style="font-size: 13px; margin-top:10px; text-align: justify; color:#555">
            Alexander Gray serves as VP of Foundations of AI at IBM, and currently leads a global research program in Neuro-Symbolic AI at IBM.  
                He received AB degrees in Applied Mathematics and Computer Science from UC Berkeley and a PhD in Computer Science from Carnegie Mellon University.  
                Before IBM he worked at NASA, served as a tenured Associate Professor at the Georgia Institute of Technology, 
                and co-founded and sold an AI startup in Silicon Valley.  His work on machine learning, statistics, and algorithms for massive datasets, 
                predating the movement of "big data" in industry, has been honored with a number of research honors including multiple best paper awards, 
                the NSF CAREER Award, selection as a National Academy of Sciences Kavli Scholar, and service as a member of the 
                2010 National Academy of Sciences Committee on the Analysis of Massive Data.  His current interests generally revolve around the 
                injection of non-mainstream ideas into ML/AI to attempt to break through long-standing bottlenecks of the field. 
        </div>
        

    </div>

    <a name="Agenda"></a>
    
    
</div>